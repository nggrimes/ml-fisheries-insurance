---
title: Finding suitable weather indices for novel fisheries index insurance using machine learning
subtitle: Working Paper not for Distribution
author: 
  - name: Nathaniel Grimes
    email: nggrimes@ucsb.edu
    affiliations:
      - id: ucsb
        name: University of California, Santa Barbara
        department: Bren school of Environmental Science and Management
        address: Street Address
        city: Santa Barbara
        state: California
    attributes:
      corresponding: true
abstract: |
 Index insurance is a financial tool gaining traction for application in fisheries. It will cover fishers losses under extreme weather events that impact fishery productivity. This is the first assessment to determine the feasibility of such programs and whether suitable indices exist. Catch and revenue data from 74 California fisheries are matched to 20 environmental variables using three prediction models: linear regression, LASSO regression, and random forests. The models are used to calculate the utility improvements of index insurance for fishers. Random forests provide the most significant improvements in utility, increasing fisher welfare by 14%. The results suggest that index insurance can improve fisher welfare, but the extent of success is not consistent across all fisheries. Specifically tailored insurance contracts must be created for each fishery using the best available data and models
keywords:
   - Index Insurance
   - Fisheries
   - Machine Learning
date: last-modified
bibliography: library.bib
csl: fish-and-fisheries.csl
toc: true
number-sections: true
format:
    pdf:
      keep-tex: true
      include-in-header:
        text: |
         \addtokomafont{disposition}{\rmfamily}
    
execute:
  echo: false
  message: false
  warning: false
---

## Introduction

Predicting fishery output from weather variables is notoriously difficult. It is widely established that climate and weather affect fishing populations [@Lehodey2006], but most stock assessment models use little to no year to year environmental data [@Privitera2020]. Variations in environmental conditions are now the leading cause of fishery closures and disaster relief payouts in the United States [@Bellquist2021]. Disaster declarations are becoming more frequent straining a slow, inequitable system [@Holland2020; @Jardine2020]. Calls for new financial tools to alleviate fisher income shocks have grown [@Mumford2009;@Sethi2010].

Index insurance has risen as a prime candidate to protect fishing communities during disasters [@Watson2023]. Index insurance is a financial product that pays out when an independently verified index, such as rainfall or temperature, falls below a predetermined threshold. The index is chosen to be highly correlated with the asset being insured. However, it is difficult to establish clear, concise weather impacts on fishery productivity. The biological dynamics of the system can lead to lower stock health persisting for years after a negative shock [@Hilborn2003]. Individual fisheries can cover enormous areas in ocean basins. The expansive spatial coverage of fisheries makes it unclear where and how much specific weather variables impact biological abundance. In addition, if weather impacts have been observed, they are most likely highly non-linear adding further complexity. The greatest impediment to the development of fishery insurance policies is reconciling these challenges to find suitable indices that can predict fishery productivity [@Watson2023]. 

Recent expansions in oceanic remote sensing has led to a plethora of new environmental indices that could be used to predict fishery productivity. Fishery data collection continues to improve with better reporting systems with longer and more detailed catch histories. This study aims to leverage these improved data sources to create suitable indices for fisheries index insurance using machine learning.

The difficulty in modelling fishery productivity with environmental indices leads to basis risk. Formally, basis risk is the probability that policyholders experience a harmful shock to their income, but the index does not trigger. Basis risk lowers demand for index insurance and remains a significant roadblock in setting up new programs [@Clarke2016; @binswanger2012;@Clement2018]. 

It is impossible to completely eliminate basis risk, and there exists a wide range in exisiting agriculture products. Well designed policies can capture up to 90% of the income variation as shown in Kenyan pasture grazing indices [@Jensen2019]. Abysmal correlations are prevalent in the Rainfall Index Insurance for Pasture, Rangeland, and Forage (RI-PRF) program where correlations as low as 0.071 exist in California, which leads to 46% additional points of basis risk [@Keller2022]. The program has a 26% probability of not paying out when damages are suffered in Nebraska and Kansas [@Yu2019]. Subsidies covering up to 60% of ranchers paid premiums are needed to stimulate demand in the RI-PRF program [@Goodrich2019].

Contract design can mitigate basis risk through providing more options so that individuals can better select policies that protect them. Policyholders choose lower trigger levels when correlations between between index and asset are low [@Lichtenberg2022]. Lower trigger levels correspond to protection against more catastrophic shocks. Increased contract flexibility reduces basis risk by only small amounts. @Yu2019 found that more flexible contracts could account for only 5-9% of basis risk. Farms in Kansas closer to weather stations had better predictive impacts of rain on yield [@Yu2019].

Designing indices with stronger correlations to fishery losses is the most effective way to reduce basis risk [@Jensen2019]. Agricultural researchers continually seek new methods and data sources to improve the correlation between loss and weather variables. Quantile regressions improve Kazak wheat farmers utility between 0.1-22% over linear models depending on the underlying measure of utility [@Conradt2015]. Remote sensing variables leveraging the latest satellite data on vegetative cover and rainfall provide better coverage than county wide averages [@arias2020;@Dalhaus2016].  

Machine learning has exploded as a new method to define better indices in agriculture index insurance [@Cesarini2021;@Feng2019;@Chen2024;@Schmidt2022]. Machine learning models excel in index insurance because indemnity contracts only need predictive relationships. Whereas, fishery stock assessments build complex models with biological foundations to accurately inform management of future fish stocks, index insurance can look retroactively at data to uncover relationships and test out of sample predictive quality. Machine learning may be necessary in fisheries index insurance to uncover any valuable relationships between weather and productivity.

The application of machine learning is growing in fisheries as researchers explore data questions beyond formal stock assessments. Ensemble models built through combinations of random forests, boosted trees, and dynamic linear models improved Bristol Bay sockeye salmon forecasts by 15% compared to a standard lagged regression model [@Ovando2022]. Environmental variables of importance to groundfish populations in Alaska were uncovered using single index varying coefficient models regularized with LASSO [@Correia2021]. Random Forests models better predict fish catch in Indonesia than traditional linear models [@Rahman2022]. The expected non-linear interactions of weather and fishery productivity merit the use of machine learning in fisheries.


This study will provide the first comprehensive examination of the necessary features of weather indices for fisheries index insurance. We contribute to the growing ocean adaption and blue finance literature. While our intent is to improve fisher welfare with a new financial program, we also expand the assessment of which weather variables have predictive performance in highly productive upwelling ecosystems. 

The rest of the paper is structured as follows. @sec-model describes the insurance model tested in this study. @sec-data describes the data collection, transformations, and sources. Fisheries data comes from newly open-access sources provided by the California Department of Fish and Wildlife. @sec-methods describes the algorithms used to predict fishery productivity and evaluate the utility of index insurance. @sec-results presents the welfare results as well as extracting which variables contribute to the prediction models to infer more interpretable results for fishers. @sec-discussion discusses the results and implications for the future of fisheries index insurance.

## Insurance Model {#sec-model}

Insurance contracts are specified by calculating payout functions ($I(\omega)$) based on independently measured weather variables. Neural networks have been used to provide non-linear payoff schedules that more effectively reduce basis risk than linear models [@Chen2024]. While it has been shown that linear payoff functions inherently lead to basis risk and therefore lower demand [@Clarke2016;@Jensen2016], we maintain their use to preserve measures of interpretability that are clearer for a first analysis of fishery index insurance.

Payouts will be issued when models predict negative deviations from the long run average. The three prediction models ($k\in\{\text{LR,LA,RF}\}$ are a linear regression \($\text{LR}$\), a LASSO regression ($\text{LA}$), and a random forest ($\text{RF}$). 

$$
I(\omega,l,c)=\max(0,(\bar{\pi}-\hat\pi_t^k(\omega)\cdot c) \cdot l)
$${#eq-payout}

Where $k$ is the prediction model, $l$ is the level of scale, $c$ is the coverage, $\hat\pi_t^k(w)$ is the predicted fishing variable from $\omega$ weather variables, and $\bar{\pi}$ the long run average of the fishing variable. Scale is the amount of protection in unit loss a policyholder chooses to protect protect relatively to the long term average, and coverage is the deviation from the index that initiates a payout. For example, when $c=1$, payouts are distributed anytime the index falls below the long run average. Lower values of $c$ require larger disasters to trigger payouts. Both variables are often chosen by policyholders when purchasing insurance. Coverage is usually constrained as set choices e.g. $c\in\{0.7,0.85,1\}$, whereas scale is a continuous choice. The premium $\rho$ is calculated as the expected value of the payout function times the premium loading factor $m$ (@eq-premium).

$$
\rho(\omega)=\mathbb{E}[I(\omega,l,c)]m
$$ {#eq-premium}

Utility measures offer the most insightful evaluation of index insurance policies [@Kenduiywo2021]. It captures value added for policyholders, not just measures of payout frequency as other measures of basis risk. We use log utility measures as our base case for constant relative risk aversion, and use exponential utility as robustness checks to validate results. Fishing variables may vary extensively from fishery to fishery. We normalize utility by dividing all measured payouts and fishing variables by the maximum observed value. Expected utility for a given fishery is the average utility over all years in the sample for any variable of interest $\pi_t$. Fishers choose insurance scale $l$ to maximize expected utility over the time period for an offered insurance contract built on the models. Two coverage levels are provided exogenously to fishers, $c\in\{0.7,1\}$ to test whether fishers are better off with more frequent payouts or disaster coverage.

$$
\begin{aligned}
\mathbb{E}[U_{b}]&=\frac{1}{n}\sum_{t}^{T}u(\pi_t) &\text{No Insurance}\\
\mathbb{E}[U_{i}]&=\max_{l}\frac{1}{n}\sum_{t}^{T}u((\pi_t+I(\omega,l,c))-\rho(w)) &\text{Insurance}\\
U_{r}&=\frac{\mathbb{E}[U_i]-\mathbb{E}[U_b]}{\mathbb{E}[U_b]}\cdot100 &\text{Percent Change in Utility}\\
\end{aligned}
$$ {#eq-utility}


We will compare the percent change in fisher utility with insurance ($U_i$) versus without insurance ($U_{b}$) for each prediction method. The variable of interest $\pi_t$, will be fishing revenue, landings, and revenue per fisher to test what measures of fishery productivity are most suitable for index insurance.

No data on fishery insurance suppliers exists to create a market equilibrium. We iteratively vary the premium loading $m\in[0.8,2]$ to examine fisher willingness to pay for insurance with a given $m$. Then, the premiums indicate the expected yearly revenue an insurance company could expect. Insurance companies could then examine their own administrative and legal costs to determine whether the feasible contract is profitable. Loading factors below one represent subsidies to the fishers. In reality, the fisher would pay the given price, but the subsidy would be split between the fisher and the insurance company. The subsidy payments are modeled only to examine the improvements in welfare offered to fishers and whether it can overcome poor model performances.

## Data {#sec-data}

This study attempts to cover breadth, not depth in possible indices. Each fishery has unique ecological characteristics that interact with environmental variables in different and non-linear ways. By studying a wide collection of fisheries and environmental variables we can uncover the potential feasibility of index insurance for fisheries holistically, and then further refine measures with ecologically sound models in future applications. 

### Fishery Data

Landings revenue, and participation data comes from the West Coast Fish data package [@Free2022]. It is a reconstruction of California Fish and Wildlife Department catch data combined with PacFin receipts for Washington and Oregon. The last three years of data are updated from the CDFW Marine Fisheries Data Explorer (MFDE). Names are matched to each species within the West Coast Fish data package.

We select California fisheries with a minimum of 30 years of consecutive, non-confidential catch records at both the state and port-complex level. Unclassified catch records are dropped i.e. "Other Sharks" and similar categories. Fisheries with an average revenue from 2010-2019 greater than \$100,000 at the state and \$75,000 at port-complex level are analyzed. Twenty four fisheries at the state level and 50 fisheries at the port complex level meet these criteria. These fisheries contain the most economically important fisheries in California and their mean values are shown in @tbl-fish-sum and at the port complex in @tbl-fish-port-sum.


Fisheries have complex spatial dynamics. Agriculture has clear, quantifiable impacts of weather in grids that are well suited for index insurance. Drought on a single farm directly leads to crop loss for that farm. Whether there is sufficient spatial coverage to identify impacts down to an individual farm remains a challenge in agriculture [@Leppert2021;@Dalhaus2016;@Stigler2024]. Fish and fishers can move thousand of miles in a given year, thus more consideration must be given to the location of weather impacts in fisheries. We spatially refine catch histories using the California CDFW fishing blocks records from the MFDE Data Explorer. Summarized catch histories of all landed fish within each block provide an average representation of effort for a given fishery. Spatial catch history is measured at both the state and port-complex level. The spatial location refines the location of environmental variables. Local weather is more likely to affect fishery productivity and catch than observations thousands of miles away.



### Environmental Data

Fisheries are highly sensitive to marine heatwaves and water temperature. Sea surface temperature is a natural variable to first consider in fisheries index insurance. Sea surface temperature data comes from the NOAA DHW data set that provides 5-km resolution of monthly temperature from 1985 to 2023. The 5-km grids are averaged within the nearest California fishing block to provide an annual time series of temperature for each fishery. Temperature is lagged from 1 to 3 years prior to account for residual impacts that carry over due to fishery biological dynamics.

Upwelling provides vital nutrients to stimulate primary productivity. The coast of California is a highly productive ecosystem due to its patterns of upwelling [@Huyer1983;@Chelton1982]. We capture upwelling through monthly observations of Coastal Upwelling Transport Index (CUTI) and Biological Effective Upwelling Transport Idnex (BEUTI). Both indices create measures of vertical movement in the mixed layer at 1 degree latitude intervals extending 75 km along the entire US West Coast [@Jacox2018]. The closest layer to the surface was used in this analysis as the correlation between surface index values and deeper index values are high. CUTI examines the physical measures of wind, ekman transport, and cross-shore geostrophic transport to indicate the strength of upwelling in a given month. BEUTI adds nitrate concentration in its calculation to capture more biological effects of upwelling. Fishing blocks are matched to the nearest 1 degree latitude interval to provide a monthly time series of upwelling for each fishery. Seasonal strengths of upwelling are captured by averaging CUTI and BEUTI within each quarter of the year. Spring upwelling in early March and April are especially important to a wide array of fish species. Yearly average and amplitude values (the difference between minimum observed upwelling and maximum) are also calculated. These indices are the most temporally limited datasets in this analysis, only extending from 1988 to 2023.

The Habitat Compression Index measures the area extent of water below average temperatures thresholds along the US West Coast [@Schroeder2022]. Habitat compression is a measure of the spatial extent of cold water habitats that are important for fish species. The index is broken down into four distinct oceangraphic regions ranging from 3.5 degrees to 5.5 degrees lattitude in size with coverage out to 150 km offshore. We use the cumulative habitat compression index that sums the index value in each month to provide a yearly time series of habitat compression for each fishery. The cumulative index showed stronger correlations with biological productivity measures than monthly measures [@Schroeder2022]

The final environmental variables are the Pacific Decadal Oscillation (PDO) and the El Nino Southern Oscillation (ENSO). Both indices are well known to affect marine ecosystems and fisheries. Both indices are averaged over a given year. PDO data is taken from the PDO ERSST V5, and ENSO data is taken from the multivariate ENSO Index Version 2 (MEI.v2).

Summary statistics for the environmental data are presented in @tbl-env-sum. In total, 74 fisheries with 35 years of catch data and 20 weather variables are spatially matched with annual coverage from 1988 to 2023.

```{r}
library(tidyverse)
#load weather data
load(here::here("data","environmental","block_beuti.rda"))
load(here::here("data","environmental","block_cuti.rda"))
load(here::here("data","environmental","block_hci.rda"))
load(here::here("data","environmental","block_sst.rda"))
load(here::here("data","environmental","enso_pdo.rda"))

#load designed fucntions
source(here::here("src","fcn","cw_join_cali.R"))
```


## Methods {#sec-methods}

We use three models to predict yearly fishing revenue, landings, and catch per fisher at state and port-complex levels. Linear models are used as the base model given its ubiquitous use in index insurance policies. We compare utility improvements with the adoption of more robust LASSO regression and random forest models.

In all class of models, the final utility maximization choice of coverage leverage is found through a box constrained quasi-Newton Method using the optim function in R. Choices are constrained to be non-negative. Premium schedules are found by the model output below the trigger values in @eq-payout and then averaged over the total fishery data.

### Linear Models

Perfect regression coefficients mimic the optimal choice of scale in index insurance contracts [@Mahul1999]. Combined with the ease of implementation, linear models on single weather indices are the most common design choice for index insurance policies. They offer a basic starting place to consider the viability of fishery index insurance. 

Yearly aggregated fishing variables are regressed on each environmental variable individually. Weather variables are spatially matched to the location of catch. We perform a 10 fold cross validation method to determine the best individual weather variable based on root mean square error (RMSE). To preserve the time series element of the data, we used a rolling split to partition the training and testing data. For example, the first fold contains the first 70% of data as training (1988-2011), and the last 30% as testing (2013-2023). The final date of the training set is extended in each fold until the year 2020 to create 10 folds. Models with the lowest average RMSE are selected and trained on the full set before being passed to the utility optimization procedure in @eq-utility. 


### LASSO Regression

Least Absolute Shrinkage and Selection Operator (LASSO) regression is a popular regularization technique to assist model selection. It attempts to minimize the residual sum of squared errors through Ordinary Least Squares (OLS), but adds a penalty constraint on the absolute sum of selected coefficient values (@eq-lasso).

$$
\hat{\beta}^{lasso}=\arg\min_{\beta}\left\{\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\omega_{ij}\beta_j)^2+\lambda\sum_{j=1}^{p}|\beta_j|\right\}
$$ {#eq-lasso}

Where, $y_i$ is our fishing variable, $\beta$ the regression coefficients, $n$, the number of observations, $p$ the number of predictors, and $\omega$ the total collection of weather variables. The $\lambda$ is the penalty term that controls the amount of shrinkage. Models are trained using the `glmnet` package in R. The LASSO regression model is trained on 200 bootstrapped samples of the training data. The optimal $\lambda$ is selected through a grid search method that selects the minimum RMSE. This choice is to ensure the most parsimonious model that still captures the most important weather variables. LASSO is particularly well suited for this research design as the absolute value of the penalty term shrinks coefficients to zero. Overfitting is a concern with so few observations in the initial training set; the shrinkage towards zero will help minimize this bias by reducing the parameter space.


### Random Forests

While LASSO offers us the ability to simultaneously explore a wide collection of weather variables including lagged effects, it remains linear in its predictions. Random Forests are tree-based ensemble models that capture non-linear interactions through recursive partitioning. They are less sensitive to over fitting through the aggregation of many trees.  

We tune two hyperparameters to create the best performing random forest for each fishery: The number of variables to consider at each split and the minimum number of observations in a leaf node. We use a grid search method to find the best hyperparameters based on RMSE through the year based cross validation method presented in the linear models. The final model is trained on the full dataset and passed to the utility optimization procedure in @eq-utility.

### Weather variables of importance

Machine learning algorithms are inherently "black boxes" that sacrifice interpreability for predictive accuracy. Fishers will be less likely to purchase complicated products that do not correspond to their experiences. Extracting the relative contribution of weather variables will assist translating products to fishers. Additionally, it can help ground-truth the chosen variables with previous biological modelling. 

The cross-validation in the linear models provides a simple weather variable comparison. We calculate the frequency a given weather variable is chosen as the best performing linear model.

We use `vip` package in R to extract importance measures for both the LASSO and random forests ^[Nathan note: I need to read more exactly how this package will extract between permutations or variance measures]. Feature extraction will occur for each fishery product, and the importance of each will be normalized then aggregated in order to compare all features. 

## Results {#sec-results}

Index insurance can improve fisher utility even with univariate linear models, but the extent of success is not consistent across all fisheries. At actuarially fair premiums, $m=1$, the average utility improvement for fishers with linear models is approximately 2% at the state level of all fishing dependent variables (@tbl-utility-state).  LASSO increases the utility gains of index insurance through better performing indices. The improvements slightly depend on the fishery dependent variable. Targeting fishing revenues provides more than 1% additional gains in utility over landings. Revenues may be more difficult to predict as they incorporate external market factors such as price and demand, but revenues more closely match the financial interests of fishers.

Random forests provide the most significant improvements in utility. The ability to capture the non-linear interactions of weather variables with fishery productivity provides much more accurate trigger states that more closely align with lost value. All three fishery dependent trigger variables with random forest models improved fisher utility by 14%. 

```{r}
#| label: tbl-utility-state
#| tbl-cap: Average relative percent improvement in utility for 24 fisheries at the state level. Standard deviations in utilty are included in parathesis. Fishers

load(here::here("data","output","port_lm_output.rda"))
load(here::here("data","output","port_lasso_output.rda"))
load(here::here("data","output","cali_lm_output.rda"))
load(here::here("data","output","cali_lasso_output.rda"))
load(here::here("data","output","cali_rf_output.rda"))
load(here::here("data","output","port_rf_output.rda"))

library(kableExtra)
library(tidyverse)
library(knitr)

## calculate standard deviations of all u_rr columns from each dataset
cali_mt_sd_lm<-sd(cali_mt_lm$u_rr)
cali_mt_sd_lasso<-sd(cali_mt_lasso$u_rr)
cali_mt_sd_rf<-sd(cali_mt_rf$u_rr)

cali_rev_sd_lm<-sd(cali_rev_lm$u_rr)
cali_rev_sd_lasso<-sd(cali_rev_lasso$u_rr)
cali_rev_sd_rf<-sd(cali_rev_rf$u_rr)

cali_per_sd_lm<-sd(cali_per_lm$u_rr)
cali_per_sd_lasso<-sd(cali_per_fisher_lasso$u_rr)
cali_per_sd_rf<-sd(cali_per_rf$u_rr)

cali_tbl<-data.frame(mt=c(mean(cali_mt_lm$u_rr),
                          mean(cali_mt_lasso$u_rr),
                          mean(cali_mt_rf$u_rr)),
                     rev=c(mean(cali_rev_lm$u_rr),
                           mean(cali_rev_lasso$u_rr),
                           mean(cali_rev_rf$u_rr)),
                     per=c(mean(cali_per_lm$u_rr),
                           mean(cali_per_fisher_lasso$u_rr),
                           mean(cali_per_rf$u_rr)))|> 
  mutate(across(where(is.numeric),round,1))

cali_tbl$mt[1]<-paste0(cali_tbl$mt[1],"%\n","(",round(cali_mt_sd_lm,1),")")
cali_tbl$mt[2]<-paste0(cali_tbl$mt[2],"%\n","(",round(cali_mt_sd_lasso,1),")")
cali_tbl$mt[3]<-paste0(cali_tbl$mt[3],"%\n","(",round(cali_mt_sd_rf,1),")")
cali_tbl$rev[1]<-paste0(cali_tbl$rev[1],"%\n","(",round(cali_rev_sd_lm,1),")")
cali_tbl$rev[2]<-paste0(cali_tbl$rev[2],"%\n","(",round(cali_rev_sd_lasso,1),")")
cali_tbl$rev[3]<-paste0(cali_tbl$rev[3],"%\n","(",round(cali_rev_sd_rf,1),")")
cali_tbl$per[1]<-paste0(cali_tbl$per[1],"%\n","(",round(cali_per_sd_lm,1),")")
cali_tbl$per[2]<-paste0(cali_tbl$per[2],"%\n","(",round(cali_per_sd_lasso,1),")")
cali_tbl$per[3]<-paste0(cali_tbl$per[3],"%\n","(",round(cali_per_sd_rf,1),")")

rownames(cali_tbl)<-c("Linear","LASSO","Random Forest")
kable(cali_tbl,format="latex",booktabs=TRUE, col.names = c("","Landings","Revenue","Revenue per Fisher"))
```

The range of responses varies for each fishery. Inherent volatility and model performance drive the differences. Chinook Salmon saw the largest gain in utility out of all fisheries with a random forest model triggered on revenues (@fig-cali-rev). Random forests always led to utility improvements, whereas LASSO and linear models maintained exorbitant basis risk compelling fishers to choose no insurance in Pacific Bonito, Cabezon, and California Sheephead

```{r}
#| label: fig-cali-rev
#| fig-cap: Utility improvements for California fisheries with index insurance with a revenue trigger. Random Forest models (green) provide the greatest utility imporvement relative to no insurnace. LASSO (teal) generally out performs linear models (blue).

c1<-cali_rev_rf %>% 
  mutate(name=unique(data[[1]]$comm_name)) |> 
  select(name,u_rr) |> 
  mutate(model='Random Forest')

c2<-cali_rev_lasso %>%
  mutate(name=unique(data[[1]]$comm_name)) |> 
  select(name,u_rr) |> 
  mutate(model='LASSO')

c3<-cali_rev_lm %>%
  mutate(name=unique(data[[1]]$comm_name)) |>
  select(name,u_rr) |>
  mutate(model='Linear') |> 
  mutate(u_rr=case_when(u_rr<0~0,
                        TRUE~u_rr))

rev_plt<-rbind(c1,c2,c3)

# reorder the fishery names by greatest utility improvement



rev_plt$model<-factor(rev_plt$model,levels=c("Linear","LASSO","Random Forest"))

ggplot(rev_plt,aes(x=reorder(name,u_rr),y=u_rr,fill=model))+
  geom_bar(stat="identity",position="dodge")+
  theme_minimal()+
  labs(x="",
       y="Utility Improvement (%)",
       fill="Model")+
  scale_fill_manual(values=c("#003660","#09847A","#79A540"))+
  coord_flip()
```

Different triggers maintain the same magntiude of improvements, but benefit to specific fisheries can change (@fig-cali-mt).

```{r}
#| label: fig-cali-mt
#| fig-cap: Utility improvements for California fisheries with index insurance with a landings trigger. Random Forest models (green) provide the greatest utility imporvement relative to no insurnace. LASSO (teal) generally out performs linear models (blue).

c1<-cali_mt_rf %>% 
  mutate(name=unique(data[[1]]$comm_name)) |> 
  select(name,u_rr) |> 
  mutate(model='Random Forest')

c2<-cali_mt_lasso %>%
  mutate(name=unique(data[[1]]$comm_name)) |> 
  select(name,u_rr) |> 
  mutate(model='LASSO')|> 
  mutate(u_rr=case_when(u_rr<0~0,
                        TRUE~u_rr))

c3<-cali_mt_lm %>%
  mutate(name=unique(data[[1]]$comm_name)) |>
  select(name,u_rr) |>
  mutate(model='Linear') |> 
  mutate(u_rr=case_when(u_rr<0~0,
                        TRUE~u_rr))

rev_plt<-rbind(c1,c2,c3)

# reorder the fishery names by greatest utility improvement



rev_plt$model<-factor(rev_plt$model,levels=c("Linear","LASSO","Random Forest"))

ggplot(rev_plt,aes(x=reorder(name,u_rr),y=u_rr,fill=model))+
  geom_bar(stat="identity",position="dodge")+
  theme_minimal()+
  labs(x="",
       y="Utility Improvement (%)",
       fill="Model")+
  scale_fill_manual(values=c("#003660","#09847A","#79A540"))+
  coord_flip()
```


Results are consistent when looking at the port-complex level, though LASSO and random forest have 2% less utility improvements than at the state level (@tbl-utility-port). Large discrepncies can exist between port-complexes for the same insurance policies. For example, an insurance contract for Chinook Salmon based on metric tons at the California state level improved utility by 21%. However, different port complexes choose different levels of coverage (@tbl-chnk). Eureka was the most extreme difference by opting for no insurance. The weather variability was not captured sufficiently by the LASSO model in Eureka and could not provide enough smoothing to incentivize fishers to purchase insurance. This observation emphasizes the benefit of creating policies at the port-complex level as the local variations capture basis risk exposure to each unique community ^[Nathan note: Should compare the eureka port-complex data with the cali lasso model to see if they would want insurance with that. Kind of a robustness check].

```{r}
#| label: tbl-utility-port
#| tbl-cap: Average relative percent improvement in utility for 50 fisheries at the port complex level.

port_tbl<-data.frame(mt=c(mean(port_mt_lm$u_rr),
                          mean(port_mt_lasso$u_rr),
                          mean(port_mt_rf$u_rr)),
                     rev=c(mean(port_rev_lm$u_rr),
                           mean(port_rev_lasso$u_rr),
                           mean(port_rev_rf$u_rr)),
                     per=c(mean(port_per_lm$u_rr),
                           mean(port_per_fisher_lasso$u_rr),
                           mean(port_per_rf$u_rr)))|> 
  mutate(across(where(is.numeric),round,1))

port_mt_sd_lm<-sd(port_mt_lm$u_rr)
port_mt_sd_lasso<-sd(port_mt_lasso$u_rr)
port_mt_sd_rf<-sd(port_mt_rf$u_rr)

port_rev_sd_lm<-sd(port_rev_lm$u_rr)
port_rev_sd_lasso<-sd(port_rev_lasso$u_rr)
port_rev_sd_rf<-sd(port_rev_rf$u_rr)

port_per_sd_lm<-sd(port_per_lm$u_rr)
port_per_sd_lasso<-sd(port_per_fisher_lasso$u_rr)
port_per_sd_rf<-sd(port_per_rf$u_rr)

port_tbl$mt[1]<-paste0(port_tbl$mt[1],"%\n","(",round(port_mt_sd_lm,1),")")
port_tbl$mt[2]<-paste0(port_tbl$mt[2],"%\n","(",round(port_mt_sd_lasso,1),")")
port_tbl$mt[3]<-paste0(port_tbl$mt[3],"%\n","(",round(port_mt_sd_rf,1),")")
port_tbl$rev[1]<-paste0(port_tbl$rev[1],"%\n","(",round(port_rev_sd_lm,1),")")
port_tbl$rev[2]<-paste0(port_tbl$rev[2],"%\n","(",round(port_rev_sd_lasso,1),")")
port_tbl$rev[3]<-paste0(port_tbl$rev[3],"%\n","(",round(port_rev_sd_rf,1),")")
port_tbl$per[1]<-paste0(port_tbl$per[1],"%\n","(",round(port_per_sd_lm,1),")")
port_tbl$per[2]<-paste0(port_tbl$per[2],"%\n","(",round(port_per_sd_lasso,1),")")
port_tbl$per[3]<-paste0(port_tbl$per[3],"%\n","(",round(port_per_sd_rf,1),")")

rownames(port_tbl)<-c("Linear","LASSO","Random Forest")
kable(port_tbl,format="latex",booktabs=TRUE, col.names = c("","Landings","Revenue","Revenue per Fisher"))
```

## Discussion {#sec-discussion}

**Remaining things do to**

- Variable importance

- Feasibility of interprebable models

- Alter m off actuarilly fair

- Change utility models

- Make a payout frequency table

- Show the income smoothing effect as a distribution

## Appendix {.appendix}

```{r}
library(tidyverse)

load(here::here("data","fisheries","cali_catch.rda"))
load(here::here("data","fisheries","cali_port.rda"))
```



```{r}
#| label: tbl-fish-sum
#| tbl-cap: Summary statistics of catch from 1988-2023 for California fisheries.

library(kableExtra)



sum_tbl<-cali_catch %>% 
  mutate(mt_per_fisher=case_when(mt_per_fisher==Inf~0,
                              .default=as.numeric(mt_per_fisher)),
         lb_per_fisher=case_when(lb_per_fisher==Inf~0,
                                 .default=as.numeric(lb_per_fisher))) |> 
  group_by(comm_name) %>% 
  summarize(mean_landings_mt=mean(landings_mt),
            sd_landings=sd(landings_mt),
            mean_value_usd=mean(value_usd),
            sd_value=sd(value_usd),
            mean_mt_per_fisher=mean(mt_per_fisher),
            sd_per=sd(mt_per_fisher,na.rm=TRUE),
            n=mean(n_fisher),
            n_sd=sd(n_fisher)) %>% 
  mutate(across(where(is.numeric),round,1)) |> 
  mutate(mean_value_usd=scales::dollar(mean_value_usd))

sum_tbl |> 
  kable(format='latex',col.names = c("Species","Mean","SD","Mean","SD","Mean","SD","Mean","SD"),booktabs=T) |> 
kable_styling(latex_options = c("scale_down")) |>
  add_header_above(c(" ","Landings (mt)"=2,"Revenue (USD)"=2,"MT per Fisher"=2,"Number of Fishers"=2))


```


```{r}
#| label: tbl-fish-port-sum
#| tbl-cap: Summary statistics of catch from 1988-2023 for California fisheries split between species and port complex
# Make a summary table of the port complex fish data
library(kableExtra)

sum_tbl_port<-cali_port_catch %>% 
  mutate(mt_per_fisher=case_when(mt_per_fisher==Inf~0,
                              .default=as.numeric(mt_per_fisher)),
         lb_per_fisher=case_when(lb_per_fisher==Inf~0,
                                 .default=as.numeric(lb_per_fisher))) |> 
  group_by(spp_code,port_area) %>% 
  summarize(comm_name=unique(comm_name),
    mean_landings_mt=mean(landings_mt),
            sd_landings=sd(landings_mt),
            mean_value_usd=mean(revenues_usd),
            sd_value=sd(revenues_usd),
            mean_mt_per_fisher=mean(mt_per_fisher),
            sd_per=sd(mt_per_fisher,na.rm=TRUE),
            n=mean(n_fisher),
            n_sd=sd(n_fisher)) %>% 
  mutate(across(where(is.numeric),round,1)) |> 
  mutate(mean_value_usd=scales::dollar(mean_value_usd)) |> 
  drop_na(comm_name) |> 
  mutate(port_area=stringr::str_to_title(port_area)) |> 
  ungroup() |> 
  select(comm_name,port_area,everything()) |> 
  select(-spp_code)
  
sum_tbl_port |> 
  kable(longtable=T,format='latex',booktabs=T,col.names = c("Species","Port","Mean","SD","Mean","SD","Mean","SD","Mean","SD")) |> 
  add_header_above(c(" "=2,"Landings (mt)"=2,"Revenue (USD)"=2,"MT per Fisher"=2,"Number of Fishers"=2)) |> 
  collapse_rows(columns=1:2,valign="top") |> 
  kable_styling(latex_options = "scale_down")

```


```{r}
#| label: tbl-env-sum
#| tbl-cap: Summary statistics of environmental variables from 1988-2023 for California fisheries.

load(here::here("data","environmental","block_cuti.rda"))
load(here::here("data","environmental","block_beuti.rda"))
load(here::here("data","environmental","block_hci.rda"))
load(here::here("data","environmental","block_sst.rda"))
load(here::here("data","environmental","enso_pdo.rda"))


# seperate each into yearly mean, yearly sd, the temporal resoltuion and the spatial resolution

cuti_sum<-block_cuti %>% 
  drop_na() |> 
  group_by(var) %>% 
  summarize(mean=mean(value),
            sd=sd(value)) %>% 
  mutate(resolution="Monthly",
         spatial="1 degree latitude",
         source="Jacox et al., 2018") |> 
  select(-var)

rownames(cuti_sum)<-c("CUTI Amp","CUTI Avg","CUTI Fall","CUTI Summer","CUTI Spring","CUTI Winter")

beuti_sum<-block_beuti %>% 
  drop_na() |> 
  group_by(var) %>% 
  summarize(mean=mean(value),
            sd=sd(value)) %>% 
  mutate(resolution="Monthly",
         spatial="1 degree latitude",
         source="Jacox et al., 2018") |> 
  select(-var)

rownames(beuti_sum)<-c("BEUTI Amp","BEUTI Avg","BEUTI Fall","BEUTI Summer","BEUTI Spring","BEUTI Winter")

hci_sum<-block_hci %>% 
  drop_na() |> 
  group_by(var) %>% 
  summarize(mean=mean(value),
            sd=sd(value)) %>% 
  mutate(resolution="Yearly",
         spatial="1 degree latitude",
         source="Integrated Ecosytem Assessment") |> 
  select(-var)

rownames(hci_sum)<-c("Cummulative Habitat Compression Index")

sst_sum<-block_sst %>% 
  drop_na() |> 
  group_by(var) %>% 
  summarize(mean=mean(value),
            sd=sd(value)) %>% 
  mutate(resolution="Monthly",
         spatial="5x5 km",
         source="NOAA Coral Bleaching Degree Heating Week") |> 
  select(-var)

rownames(sst_sum)<-c("Average Sea Surface Temperature","Sea Surface Temperature Lag 1 Year","Sea Surface Temperature Lag 2 Years","Sea Surface Temperature Lag 3 Years","Sea Surface Temperature Lag 4 Years")

enso_sum<-enso |> 
  group_by(year) |> 
  summarize(yr_enso=mean(enso)) |> 
  ungroup() |> 
  drop_na() |> 
  summarize(mean=mean(yr_enso),
         sd=sd(yr_enso)) |> 
  mutate(resolution="Monthly",
         spatial="Regional",
         source="MEI.v2")

rownames(enso_sum)<-c("ENSO")

pdo_sum<-pdo |> 
  group_by(year) |> 
  summarize(yr_pdo=mean(pdo)) |> 
  ungroup() |> 
  drop_na() |> 
  summarize(mean=mean(yr_pdo),
         sd=sd(yr_pdo)) |> 
  mutate(resolution="Monthly",
         spatial="Regional",
         source="PDO ERSST V5")

rownames(pdo_sum)<-c("Pacific Decadal Oscillation")

env_tbl<-rbind(cuti_sum,beuti_sum,hci_sum,sst_sum,enso_sum,pdo_sum) |> 
  mutate(across(where(is.numeric),round,1))

env_tbl$variable<-c("CUTI Amp","CUTI Avg","CUTI Fall","CUTI Summer","CUTI Spring","CUTI Winter",
                     "BEUTI Amp","BEUTI Avg","BEUTI Fall","BEUTI Summer","BEUTI Spring","BEUTI Winter",
                     "Cummulative Habitat Compression Index",
                     "Average Sea Surface Temperature","Sea Surface Temperature Lag 1 Year","Sea Surface Temperature Lag 2 Years","Sea Surface Temperature Lag 3 Years","Sea Surface Temperature Lag 4 Years",
                     "ENSO","Pacific Decadal Oscillation")

env_tbl<-env_tbl %>%
  select(variable,mean,sd,resolution,spatial,source)



env_tbl |>
  kable(format='latex',col.names = c("Weather Index","Mean","SD","Temporal Resolution","Spatial Resolution","Source"),booktabs=T) |>
  kable_styling(latex_options = "scale_down")


```


## References
