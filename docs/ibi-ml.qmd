---
title: Finding suitable weather indices for novel fisheries index insurance
subtitle: Working Paper not for Distribution
author: 
  - name: Nathaniel Grimes
    email: nggrimes@ucsb.edu
    affiliations:
      - id: ucsb
        name: University of California, Santa Barbara
        department: Bren school of Environmental Science and Management
        address: Street Address
        city: Santa Barbara
        state: California
    attributes:
      corresponding: true
abstract: |
 Index insurance is a financial tool gaining traction for application in fisheries. It will cover fishers losses under extreme weather events that impact fishery productivity. This is the first assessment to determine the feasibility of such programs and whether suitable indices exist. Catch and revenue data from 74 California fisheries are matched to 20 environmental variables using three prediction models: linear regression, LASSO regression, and random forests. The models are used to calculate the maringal willingness to pay through utility models.
keywords:
   - Index Insurance
   - Fisheries
   - Machine Learning
date: last-modified
bibliography: library.bib
csl: fish-and-fisheries.csl
toc: true
number-sections: true
format:
    pdf:
      keep-tex: true
      include-in-header:
        text: |
         \addtokomafont{disposition}{\rmfamily}
    
execute:
  echo: false
  message: false
  warning: false
---

## Introduction

Fishers face enormous environmental variability that impacts their livelihoods. Extreme weather events that degrade stock health such as stagant upwelling or marine heatwaves can lead to significant losses in fishery productivity. These shocks can devastate fishing communities, leading to income loss and food insecurity [@Holland2020; @Jardine2020]. The need for financial tools to protect fishers from these shocks has never been greater [@Mumford2009,@Sethi2010@Kasperski2013;@Sumalia2020]. Index insurance has risen as a leading candidate to protect fishing communities during such disasters, but accurately identifying suitable weather indicies for fisheries has impeded widespread deployment [@Watson2023]. This paper is the first to empirically construct index insurance contracts to protect against flucuations in fishery biological productivity. 

Index insurance is a financial product that pays out when an independently verified index, such as sea surface temperature or El Nino Southern Oscillation (ENSO) measures, falls below a predetermined threshold called the trigger. The index is ideally chosen to be highly correlated with the asset being insured. In fisheries, catch or yield will be the primary underwritten asset. The trigger represents a critical value where the underlying assest is expected to suffer a loss. Fishers pay a premium that transfers income from good periods to bad periods.

The COAST program is the only extant index insurance product for fisheries [@Sainsbury2019]. It is a pilot program that insures small island nations in the Caribbean from hurricanes. It uses indicies of wave height, wind speed, and storm surge to intiate payouts once they fall below the trigger.  There is growing interest to expand index insurance to cover other environmental shocks, but significant market barriers have limited widespread implementation [@Watson2023].

The primary market barrier is empirically constructing contracts that sufficiently reduce basis risk. Basis risk is the potential for a policyholder's actual loss to not align with the contract payouts. For example, if a fisher suffers poor catch, but the weather index does not exceed the trigger, then the fisher will not receive payouts. The opposite may occur where insurance companies payout despite fishers receiving high catch. Basis risk lowers demand for index insurance and leads to exhoribant premium rates [@Clarke2016;@Binswanger2012].

Identifying triggers and indicies that are highly correlated to loss is the best way to eliminate basis risk. Agricultural index insurance has refined techniques to build and price contracts with minimal basis risk over the last 20 years. The most common design strategy is to match individual farms to the nearest meterological stations, then use linear regression to model yield-weather distributions. Premiums are then calculated based on the payout distribution of prior years for a given contract. 

However, fisheries possess more difficult data and empirical challenges than agriculture. Predicting fishery output from weather variables is notoriously difficult. It is widely established that climate and weather affect fish populations [@Lehodey2006], but even the most advance fishery stock assessment models use little to no year to year environmental data for projections [@Privitera2020]. Isolating environmental effects in fisheries usually entails estimating catch per unit effort data to reflect stock abundance then modeling a relationship through a generalized additive model. 

The spatial and temporal dynamics of fish populations also present unique challenges. Fish 

Despite these challenges, innovations in fishery and oceangraphic data collection and increased demand by fishers for assistance to combat against increasing environmental variability suggest a new opportunity has arisen to reconsider the use of index insurance in fisheries. 

This paper is the first to empirically evaluate the feasibility of index insurance for fisheries. I use the Market Squid Fishery in California as a case study to test insurance contracts built on five different prediction models with twelve weather variables could be used to improve fisher welfare through insurance. I determine fishers marginal willingess to pay for an index insurance contract using expected utility, and identify what price a contract would be viable for an insurance company to offer.

Contracts built with linear models possess basis risk that is too large to be market viable. Four linear models (Sea Surface Temperature, Upwelling, and a LASSO multivariate model) improve fisher utility when premiums are actuarilly fair, but none improve welfare at premium rates that keep the insurance fund solvent. All nonlinear models improved fisher utility by 1%-3% while remaining solvent with market premiums. The leading variable of influeunce in the nonlinear models were:.... Communicating to fishers the relative influence of each variable is crucial to build trust and transparency when using complex models.

Fishery management is crucial to sustainable and consistent fishery harvest. Management provides the primary source of risk mitgation in fisheries by evaluating fishery health and establishing rules to eliminate inherent common pool fishery problems. However, management does not cover all risks, and fishers remain particularly exposed to financial risk [@Kasperski2013]. Quota adjustments or closures promote long term risk protection, but lead to short term financial losses that negatively impact fishing communities. Index insurance can close this gap by providing immediate financial relief when management needs to reduce fishing pressures in response to environmental shocks. For example, while testing the viability of an area-yield insurance program for the Bristol Bay Sockeye Fishery, @Herrmann2004 concluded that insurance would not sufficiently protect fisher loss and that more management was necessary to protect fisheres. Twenty years later, the Bristol Bay Sockeye Fishery is one of the most comprehensively managed and studied fisheries in the world, but environmental shocks continue to lead to financial losses for fishers^[Granted, the flucuations are nowhere near as severe in the past during the study @Herrman2004 study window.]. 

Demand for more protection from fishers have been growing even in the best managed fisheries as the current financial tools, such as the Federal Fishery Disaster Relief Program, have become strained [@Bellquist2021]. The Federal Fishery Disaster Relief Program takes years to provide fishers with relief and has become unpopular. It is also inequitable as it does not cover all fisheries and often leaves out the most vulnerable fishers [@Holland2020; @Jardine2020]. Interviews with fishers in California have shown that fishers are willing to pay for insurance to protect against income shocks from environmental variability [TNC Study].

The explosion of environmental and fisheries data in the 21st century has broken the second market barrier. To identify catch-index relationships, sufficient time series of both indices and catch are necessary. Standardized catch reporting is widely and publically available for thousands of fisheries [@Free2022;@Ram]. Satellite data has montiored global sea surface temperature at daily 5x5 km resolution since 1985. Paired with wind measures, sea level height, and other datasets allows for the creation of new indicies that can capture regional environmental events such as upwelling or heatwaves [@Jacox2018]. Fishery independent surveys, such as the Rockfish Recruitment and Ecosystem Assessment Survey, also provide decades of standarized biological information.

New data sources are being rapidly established to provide greater insights. Global Fishing Watch provides real time monitoring of fishing events from every industrial vessel through the use of VIMS satellite data. AI assisted video identification software is being experimented with to enable managers to record fish catch at the species level while vessels are at sea. While these new sources do not have sufficently long temproal records to be used immediately, they represent the new frontier of data availability to provide accurate index insurance assessments.

Supplier knowledge gap remains the greatest market barrier. Insurance companies have cultivated expertise and an effective collection of tools to design agriulcutral index insurance. However, fisheries are a unique and complex system such that expertise in agriculture does not transfer one to one. While there are regulatory and ethical considerations unique to fisheries, in this paper, I will focus solely on addressing data and methodologcial challenges^[@Watson2023 provides a much deeper discussion on regulatory issues for interested readers].

Empirically defining the yield-index relationship is the crucial step in index insurance design. Poorly specified indices lead to basis risk, which is the primary barrier to index insurance uptake in agriculture [@Clarke2016; @binswanger2012;@Clement2018]. Basis risk is the probability that policyholders experience a harmful shock to their income, but the index does not trigger. 

Predicting fishery output from weather variables is notoriously difficult. It is widely established that climate and weather affect fish populations [@Lehodey2006], but most stock assessment models use little to no year to year environmental data [@Privitera2020]. Emprically translating the biological dynamics of fisheries into a predictive model with minimal sufficient basis risk will be a challenge. Specifically, there are four main factors that insurance companies must understand when designing index insurance for fisheries: 1) definition of loss in a fishery context, 2) ecological complexity of fisheries, 3) biological dynamics of fish populations, and 4) management. 

In this paper, I will discuss how these factors impact the design of index insurance and possible solutions to address each. I use three case studies of commercially important fisheries in the United States to demonstrate how effective index insurance can be to protect fishers while remaining solvent for the insurance companies. 

## Factors

In this section, I identify the unique fishery factors that actuarialists must consider when designing index insurance. I attempt to translate the current practices in agriculture insurance to fisheries to highlight the discrepencies and limitations that arise. 

### Definition of Loss

Agriculture has long historical records on crop losses. Farmers plant set amounts of crop that equate to a maximum potential yield. Weather impacts the harvest leading to realized yield. The difference between maximum potential yield and realized yield is yearly loss. Farmers report yearly loss to authorities to provide county/area level information [@Tack2015]. 

There is no equivalent framing in fisheries. When fishers deploy their gear, they are not guaranteed a catch. Does the discrepancy in what they expect to catch and realized catch arise from the fisher's skill, the weather, or the underlying fish population? The question of identifying loss in fisheries was a leading reason for the denial of a salmon fishery insurance in Alaska in the early 2000s [@Herrmann2004].

Individual loss is not feasible for identifying weather loss to fishing loss. 

### Ecological Complexity

### Biological Dynamics

### Management

Management

The biological dynamics of the system can lead to lower stock health persisting for years after a negative shock [@Hilborn2003]. Individual fisheries can cover enormous areas in ocean basins. The expansive spatial coverage of fisheries makes it unclear where and how much specific weather variables impact biological abundance. In addition, if weather impacts have been observed, they are most likely highly non-linear adding further complexity. If index insurance is to effectively protect fishers while remaing solvent for insurance companies, addressing these issues empirically through data is paramount. 

Index insurance rose in popularity in agriculture to offer protection for farmers and pastoralists where traditional crop indemnity insurance was not feasible [@Jensen2017]. By not requiring individual loss assessment, index insurance is cheaper to administer, faster to payout, and immune to "chasing the trigger" moral hazards. It is also accounts for correlated losses across regions and individuals, which is an expected problem in fisheries.  
Though agricultural IBI programs have seen succesful implemenation in specific context [@Jensen2016], IBI has not been the risk management panacea most envisionaged upon its inception. The primary impediment to index insurance uptake is basis risk. Formally, basis risk is the probability that policyholders experience a harmful shock to their income, but the index does not trigger. Basis risk lowers demand for index insurance and remains a significant roadblock in setting up new programs [@Clarke2016; @binswanger2012;@Clement2018]. Basis risk primarily oringates from mispecified indicies that do not accurately capture the underlying asset's loss.

Predicting fishery output from weather variables is notoriously difficult. It is widely established that climate and weather affect fishing populations [@Lehodey2006], but most stock assessment models use little to no year to year environmental data [@Privitera2020]. Variations in environmental conditions are now the leading cause of fishery closures and disaster relief payouts in the United States [@Bellquist2021]. Disaster declarations are becoming more frequent straining a slow, inequitable system [@Holland2020; @Jardine2020]. Calls for new financial tools to alleviate fisher income shocks have grown [@Mumford2009;@Sethi2010].

Index insurance has risen as a prime candidate to protect fishing communities during disasters [@Watson2023]. Index insurance is a financial product that pays out when an independently verified index, such as rainfall or temperature, falls below a predetermined threshold. The index is chosen to be highly correlated with the asset being insured. However, it is difficult to establish clear, concise weather impacts on fishery productivity. The biological dynamics of the system can lead to lower stock health persisting for years after a negative shock [@Hilborn2003]. Individual fisheries can cover enormous areas in ocean basins. The expansive spatial coverage of fisheries makes it unclear where and how much specific weather variables impact biological abundance. In addition, if weather impacts have been observed, they are most likely highly non-linear adding further complexity. The greatest impediment to the development of fishery insurance policies is reconciling these challenges to find suitable indices that can predict fishery productivity [@Watson2023]. 


The difficulty in modelling fishery productivity with environmental indices leads to basis risk. Formally, basis risk is the probability that policyholders experience a harmful shock to their income, but the index does not trigger. 

It is impossible to completely eliminate basis risk. Well designed policies can capture up to 90% of the income variation as shown in Kenyan pasture grazing indices [@Jensen2019]. Abysmal correlations are prevalent in the Rainfall Index Insurance for Pasture, Rangeland, and Forage (RI-PRF) program where correlations as low as 0.071 exist in California, which leads to 46% additional points of basis risk [@Keller2022]. The program has a 26% probability of not paying out when damages are suffered in Nebraska and Kansas [@Yu2019]. Subsidies covering up to 60% of ranchers paid premiums are needed to stimulate demand in the RI-PRF program [@Goodrich2019].


Designing indices with stronger correlations to fishery losses is the most effective way to reduce basis risk [@Jensen2019]. Agricultural researchers continually seek new methods and data sources to improve the correlation between loss and weather variables. Quantile regressions improve Kazak wheat farmers utility between 0.1-22% over linear models depending on the underlying measure of utility [@Conradt2015]. Remote sensing variables leveraging the latest satellite data on vegetative cover and rainfall provide better coverage than county wide averages [@arias2020;@Dalhaus2016].  

Machine learning has exploded as a new method to define better indices in agriculture index insurance [@Cesarini2021;@Feng2019;@Chen2024;@Schmidt2022]. Machine learning models excel in index insurance because indemnity contracts only need predictive relationships. Whereas, fishery stock assessments build complex models with biological foundations to accurately inform management of future fish stocks, index insurance can look retroactively at data to uncover relationships and test out of sample predictive quality. Machine learning may be able to capture catch and weather relationships in fisheries that traditional methods in index insurance contract cannot.

The application of machine learning is growing in fisheries as researchers explore data questions beyond formal stock assessments. Ensemble models built through combinations of random forests, boosted trees, and dynamic linear models improved Bristol Bay sockeye salmon forecasts by 15% compared to a standard lagged regression model [@Ovando2022]. Environmental variables of importance to groundfish populations in Alaska were uncovered using single index varying coefficient models regularized with LASSO [@Correia2021]. Random Forests models better predict fish catch in Indonesia than traditional linear models [@Rahman2022]. The expected non-linear interactions of weather and fishery productivity merit the use of machine learning in fisheries.

Recent expansions in oceanic remote sensing has led to a plethora of new environmental indices that could be used to predict fishery productivity. Fishery data collection continues to improve with better reporting systems with longer and more detailed catch histories. This study aims to leverage these advances to determine how much fishers would be willing to pay for potential realized index insurance contracts. Furthermore, this study will investigate how to effectively design insurance contracts to incentivize purchase by examining  what weather indices, fishery loss measures, and prediction models are most effective. Whether the contracts need to be subsidized or are viable in a free market will be measured by comparing the willingness to pay for insurance and payout frequency. 


The rest of the paper is structured as follows. @sec-model describes the insurance model tested in this study. @sec-data describes the data collection, transformations, and sources. Fisheries data comes from newly open-access sources provided by the California Department of Fish and Wildlife. @sec-methods describes the algorithms used to predict fishery productivity and evaluate the utility of index insurance. @sec-results demonstrates some preliminary results and highlights some of the challenges present. Future steps are outlined in @sec-discussion.

## Insurance Framework {#sec-model}

Index insurance is designed to protect policyholders from suffering lossess below critical yield thresholds. In fisheries index insurance, actuarlists must first estimate how fishery harvest is affected by weather variables. The inverse relationship between the predicted harvest and weather allows a contract to be defined in terms of the weather variables. For example, fisheries harvest, $\tilde{y}$ can be modeled with a simple technology function where fisher inputs, $x$ interact with a stock of biomass, $B$, which is a function of random weather shocks, $\tilde{w}$ as shown in equation @eq-fishprod:

$$
y=f(x,B(\tilde{w}))+\epsilon
$${#eq-fishprod}

The harvest technology function, $f(x,B(\tilde{w}))$, is increasing and concave in inputs and biomass. All other idiosyncratic shocks to harvest that are biological are captured in $\epsilon$. Assuming inputs are selected optimally conditional on the expected biomass, it is clearer to show the effects of weather on biomass and thus harvest:

$$
\tilde{y}=f(B(\tilde{w}))+\tilde{\epsilon}
$${#eq-fishprod2}

The weather and yield relationship, $f(B(\tilde w))$ can be empirically estimated by $g(\tilde{w})$, with a model error term $\eta$^[Fisheries scientists might argue that you should first estimate $b(\tilde w$) and then estimate the relationship between yield and biomass. However, that would introduce an additional model error term and is not feasible for the case study. Market squid have such short lifespans that estimating their abundance is not worthwhile. Therefore the data does not exist in our context. Also insurance companies do not have access to the confidential data sources needed to build catch per unit effort and thus biomass estimates.]. Subbing the estimator into the function gives:

$$
\tilde{y}=g(\tilde{w})+\tilde\eta+\tilde{\epsilon}
$$ {#eq-g}


Formally, basis risk is the combined stochastic effects of $\tilde\eta$ and $\tilde{\epsilon}$. The model error, $\tilde\eta$, arises from the inability to perfectly estimate the weather to yield relationship. The idiosyncratic error, $\tilde{\epsilon}$, arises from all other shocks to yield that are not related to weather. Well designed contracts minimize the negative effects of design risk stemming from $\tilde\eta$. For example, a perfectly estimated weather to harvest relationship would minimze $\tilde\eta$ to zero. 

The estimation of $g(\tilde{w})$ is extremely difficult in fisheries. However, the estimation does not need to be perfect or even statistically significant to provide value to fishers. The model just needs to capture enough of the weather to harvest relationship to improve fisher welfare. 

The four models are presented in more detail in @sec-mods. For now, the general model output will be represented as $\hat{y}(w)$, where $w$ is the observed weather variables and $\hat{y}$ is the predicted harvest. Insurance contracts can now be built off these models.


Index insurance contracts are designed as put options that initate payouts once a predicted models' estimates, $\hat{y}(w)$, fall below a critical yield threshold, $c*\mathbb{E}[y]$. 

$$
\gamma(w)=\max(c*\mathbb{E}[y]-\hat{y}(w),0)
$${#eq-gamma}

@eq-gamma indicates fishers will receive the difference between the long run average of catch and the model predicted outcome from observed weather variables. The coverage level,$c$, represents the level of protection offered by the contract.  I will test two coverage levels, 70% and 100%, where the 70% represents disaster coverage that pays out only during extreme events, and 100% which pays more frequently.

Premiums are calculated by determing the expected payout of the contract times a premium loading factor, $m$ as shown in equation @eq-prem.

$$
\rho(w)=\mathbb{E}[\gamma(w)]m
$${#eq-prem}

I use burn rate analysis to calculate the expected payout in @eq-prem. Burn rate analysis uses past realizations of weather variables to create a distribution of payouts that would have been observed if the policy was active over that time. The average payout is then used to calculate the expected payout of the contract. Burn rate analysis is a common method used by actuarialists to price index insurance contracts and is suitable in this setting because of the short time period of analysis. 

## Results

The mutlivariate models better predict weather-harvest dependency than univariate models (@fig-rmse). The highest variation explained in the training sets by a single variable was krill prey abundance (21%). However, the predictive accuracy of krill drops precipitously in the test set as krill had the highest RMSE out of all models. 

Every multivariate model greatly increased the explained variation in the training sets, but demonstrate overfitting with proportionally higher increases in the error rates than univariate models when moving to the test set. 

```{r}
#|label: fig-rmse
library(patchwork)

p1<-models |> 
  mutate(group=c(rep('Linear Regression',12),rep('Machine Multivariate',4))) |> 
  ggplot(aes(x=fct_reorder(pred_mod,group),y=rmse,fill=group))+
  geom_col()+
  labs(x='Model',y='RMSE',fill='')+
  theme_classic()+
  theme(axis.text.x = element_text(angle=45,hjust=1),)+
  scale_y_continuous(expand=c(0,0))+
  scale_fill_manual(values=c("#003660","#047C90"))

p2<-models |> 
  mutate(group=c(rep('Linear Regression',12),rep('Multivariate',4))) |> 
  ggplot(aes(x=fct_reorder(pred_mod,group),y=rmse_test,fill=group))+
  geom_col()+
  labs(x='Model',y='RMSE',fill='')+
  theme_classic()+
  theme(axis.text.x = element_text(angle=45,hjust=1),)+
  scale_y_continuous(expand=c(0,0))+
  scale_fill_manual(values=c("#003660","#047C90"))

p1/p2+plot_annotation(tag_levels = 'A')+plot_layout(axes='collect')
```


Examining fisher utility provides clearer measures of whether a model sufficiently protects fishers from environmental risk. The majority of univariate models made fishers worse off as the fishers experienced high degrees of downside risk where they did not receive payouts when necessary (@fig-ur-lr Panel A). Only three weather indices improved fisher utility: average sea surface temperature (1.5%), relaxation of upwelling (4.3%), and the frequency of upwelling events (16%). However, each of the indicies were highly unprofitable for insurance companies as each had a loss ratio greater than 250% (@fig-ur-lr Panel B), which implies that insurance companies would have to pay out \$2.50 for every \$1 of premium collected. Each indices paid large sums at times when fishers did not experience loss. The large payouts act more as wealth transfers than risk protection. 

Every multivariate model improved fisher utility while reducing the insurance company loss ratio (@fig-ur-lr) compared to the univariate models. LASSO regression provided the highest improvement in utility at 13%, but had the highest loss ratio at 167%. The random forest models balanced insurance solvency and fisher utility the best. The loss ratios for both models were 99% and 103% using actuarially fair rates. Utility gains were small at 0.8% and 2.6%. The support vector machine model improved fisher utility by 9% with

```{r}
#| label: fig-ur-lr
ur_p<-models |> 
  mutate(group=c(rep('Linear Regression',12),rep('Machine Learning',4))) |> 
  ggplot(aes(x=fct_reorder(pred_mod,group),y=u_rr,fill=group))+
  geom_col()+
  labs(x='Model',y='Percent increase in utility with insurance',fill='')+
  theme_classic()+
  theme(axis.text.x = element_text(angle=45,hjust=1),)+
  scale_y_continuous(expand=c(0,0),labels=scales::percent)+
  scale_fill_manual(values=c("#003660","#047C90"))+
  geom_hline(yintercept=0,size=0.5,color='grey')

lr_p<-models |> 
  mutate(group=c(rep('Linear Regression',12),rep('Machine Learning',4))) |> 
  ggplot(aes(x=fct_reorder(pred_mod,group),y=lr,fill=group))+
  geom_col()+
  labs(x='Model',y='Insurance Loss Ratio',fill='')+
  theme_classic()+
  theme(axis.text.x = element_text(angle=45,hjust=1),)+
  scale_y_continuous(expand=c(0,0))+
  scale_fill_manual(values=c("#003660","#047C90"))+
  geom_hline(yintercept=1,size=1,color='black')

ur_p/lr_p+plot_annotation(tag_levels = 'A')+plot_layout(axes='collect',guides="collect")

```


While contracts built with these models would see positive demand based on the increases in utility, supply would collapse as insurance companies would not be profitable. Insurance companies would need to charge higher premiums to maintain solvency. @fig-urm-lrm shows how much utility fishers would gain if insurance companies charged premiums that kept them solvent at a loss ratio of 1. The percent increase in utility is shown in Panel A, and the loading factor is shown in Panel B. The linear models require high premiums to remain solvent, which eliminates any utility gains. Only the upwelling indices provide positive utility at high premium rates. The nonlinear machine learning models provide positive utility gains at premium rates that keep insurance companies solvent. The support vector machine allows insurance companies to charge premiums 

```{r}
#| label: fig-urm-lrm

urm_p<-models |> 
  mutate(group=c(rep('Linear Regression',12),rep('Machine Learning',4))) |> 
  filter(u_rr>0) |> 
  ggplot(aes(x=fct_reorder(pred_mod,group),y=u_rr_m,fill=group))+
  geom_col()+
  labs(x='Model',y='Percent increase in utility with insurance',fill='')+
  theme_classic()+
  theme(axis.text.x = element_text(angle=45,hjust=1),)+
  scale_y_continuous(expand=c(0,0),labels=scales::percent)+
  scale_fill_manual(values=c("#003660","#047C90"))+
  geom_hline(yintercept=0,size=0.5,color='grey')

lrm_p<-models |> 
  mutate(group=c(rep('Linear Regression',12),rep('Machine Learning',4))) |> 
  filter(u_rr>0) |>
  ggplot(aes(x=fct_reorder(pred_mod,group),y=lr_m,fill=group))+
  geom_col()+
  labs(x='Model',y='Loading Rate (m)',fill='')+
  theme_classic()+
  theme(axis.text.x = element_text(angle=45,hjust=1),)+
  scale_y_continuous(expand=c(0,0))+
  scale_fill_manual(values=c("#003660","#047C90"))+
  geom_hline(yintercept=1,size=1,color='black')

urm_p/lrm_p+plot_annotation(tag_levels = 'A')+plot_layout(axes='collect',guides='collect')
```

```{r}
# show the payout schedule compared to actual catch

pay<-models |> 
  filter(pred_mod=='svm') |> 
  select(payout_vec) |> 
  unlist()

prem<-models |> 
  filter(pred_mod=='svm') |> 
  select(prem_vec) |> 
  unlist()

max_len <- max(length(df$fish_value), length(pay))

# Pad each vector
fish_value <- c(df$fish_value, rep(NA, max_len - length(df$fish_value)))

net_p <- c(rep(NA, max_len - length(pay)),pay-prem)

ins_value<-net_p+fish_value

data.frame(ins_value=ins_value,fish_value=fish_value,year=df$year) |> 
  ggplot()+
  geom_line(aes(x=year,y=fish_value),color='#003660')+
  geom_point(aes(x=year,y=fish_value,color='Harvest'))+
  geom_point(aes(x=year,y=ins_value,color='Net'),size=2)+
  theme_classic()+
  scale_color_manual(values=c(Harvest='#003660',Net='#900C3F'),labels=c("Harvest"="Harvest","Net"="Net Harvest with Insurance"),name="")+
  geom_hline(yintercept=mean(df$fish_value))+
  labs(x='',y='Harvest per fisher (MT)')+
  theme(legend.position = "bottom")


```


### Utility evaluation

The seminal work by @Clarke2016 proves the theoretical impacts of basis risk on insurance demand. Policyholders will choose non-zero coverage if the expected claim payment conditional on incurring a loss is higher than the paid premium (Equation 9 of @Clarke2016). @Clarke2016 proceed to use data from 270 insurance products in India to show that Indian farmers would be willing to pay a premium with a loading factor of up to 1.56 above the actuarailly fair rate for the product despite low Pearson correlation coefficients. His model uses explicit measures of crop loss and realized payment data. This presents two challenges in fisheries settings. First, the definition of loss arising from weather variability is not as clear in fisheries as it is agriculture. Second, there are no existing fishery index insurance products to calculate realized losses and payments to fit Clarke's ratio. 

Agriculture has long historical records on crop losses. Farmers plant set amounts of crop that equate to a maximum potential yield. Weather impacts the harvest leading to actual yield. The difference between maximum potential yield and actual yield is yearly loss. Farmers report these numbers to authorities to provide county/area level information [@Tack2015]. There is no equivalent framing in fisheries. When fishers deploy their gear, they are not guaranteed a catch. Does the discrepancy arise from the fisher's skill, the weather, or the fish population? The question of identifying loss in fisheries was a leading reason for the denial of a salmon fishery insurance in Alaska in the early 2000s [@Herrmann2004]. 

I explore multiple definitions of loss to uncover the most viable options moving forward in fisheries. Each measure of loss revolves around defining a productivity measure of a fishery $\pi$, such as catch or revenue. Setting the threshold for loss for each of the productivity measures is the next challenge. Average historical harvest is the most common choice in agriculture. I will use this as the baseline threshold for each $\pi$.  The biological dynamics of fisheries and residual impacts of weather variables suggest fisheries may need time dependent thresholds. A moving average over the last 5 years of $\pi$ could capture more accurate measures of loss. Finally, biological thresholds could be used as limits to define loss [@Deng2008]. For example, fisheries that operate below maximum sustainable yield could be considered to have incurred a loss.

To tackle the second challenge, I blend the model of @Clarke2016 with the data driven utility models of @Conradt2015 and @Kenduiywo2021. Utility measures are used to evaluate the effectiveness of index insurance policies while providing equatable measures of welfare improvement across models and settings. @Kenduiywo2021 define a ratio on the relative improvement of utility for policyholders between a theoretical "perfect" insurance contract with no basis risk and a contract with observed basis risk. @Conradt2015 compare the relative improvement in utility using an out of sample prediction approach for consistent comparison between linear and quantile regression models and different utility models. 

I will determine the marginal willingness to pay for insurance for a given contract in a fishery by finding the premium loading factor that equates the utility of having insurance with the utility of not having insurance.

$$
\mathbb{E}[U_{ni}(\pi)]=\mathbb{E}[U_{i}(\pi,I(\omega),\rho)]
$$ {#eq-ut}

The expected utility of not having insurance, $\mathbb{E}[U_{ni}]$, is the average utility over all years in the sample for any variable of interest $\pi$. The expected utility of having insurance, $\mathbb{E}[U_{i}]$, includes the contract payout schedule based on models built with weather variables, $I(\omega)$. The premium $\rho$ is calculated as the expected value of the payout function times the premium loading factor. The premium loading factor will be varied to equate both states of the world to represent the marginal willingness to pay for insurance. Essentially, it reflects the highest premium a policyholder would be willing to pay for a contract with a certain level of basis risk.
 
Insurance contracts are specified by calculating payout functions ($I(\omega)$) based on independently measured weather variables. Contracts need productivity losses and thresholds to effectively compensate policyholders. Payouts will be issued when models predict deviations from defined thresholds. The three prediction models ($k\in\{\text{LR,LA,RF}\}$ are a linear regression \($\text{LR}$\), a LASSO regression ($\text{LA}$), and a random forest ($\text{RF}$). Example thresholds include deviations from long run average (@eq-payout), a $j$ year moving average (@eq-mavg), or a biological threshold (@eq-biot).

$$
I(\omega,l,c)=\max(0,(\bar{\pi}\cdot c-\hat\pi_t^k(\omega)) \cdot l)
$${#eq-payout}


$$
I(\omega)=\max(0,(\frac{1}{j}\sum^n_{i=n-j+1}\pi_t\cdot c-\hat{\pi}_t^k(\omega))\cdot l)
$${#eq-mavg}

$$
I(\omega)=\max(0,(\pi_t(b_t)\cdot c-\hat{\pi}_t^k(b_t,\omega))\cdot l)
$$ {#eq-biot}

Where $k$ is the prediction model, $l$ is the level of scale, $c$ is the coverage, $\hat\pi_t^k(w)$ is the predicted fishing variable from $\omega$ weather variables. Scale is the amount of protection in unit loss a policyholder chooses to protect protect, and coverage is the deviation from the index that initiates a payout. For example, when $c=1$, payouts are distributed anytime the index falls below the long run average in @eq-payout. Lower values of $c$ require larger disasters to trigger payouts. Both variables are often chosen by policyholders when purchasing insurance. Coverage is usually constrained as set choices e.g. $c\in\{0.7,0.85,1\}$, whereas scale is a continuous choice. The premium $\rho$ is calculated as the expected value of the payout function times the premium loading factor $m$ (@eq-premium). The premium loading factor is the variable that will be adjusted to equate utility states. Values above 1 indicate high willingness to pay for insurance. Values below 1 imply subsidies will be needed to stimulate demand.

$$
\rho(\omega)=\mathbb{E}[I(\omega,l,c)]m
$$ {#eq-premium}

We use log utility measures as our base case for constant relative risk aversion, and use exponential utility as robustness checks to validate results. Fishing variables may vary extensively from fishery to fishery. We normalize utility by dividing all measured payouts and fishing variables by the maximum observed value in each fishery. Expected utility for a given fishery is the average utility over all years in the sample for any variable of interest $\pi$. Fishers choose insurance scale $l$ to maximize expected utility over the time period for an offered insurance contract built on the models. Two coverage levels are provided exogenous to fishers, $c\in\{0.7,1\}$ to test whether fishers are better off with more frequent payouts or disaster coverage.

$$
\mathbb{E}[U_{i}]=\max_{l_t}\frac{1}{n}\sum_{t}^{T}u(\pi_t+I(\omega,l_t,c)-\rho(w))
$$ {#eq-utility}



## Data {#sec-data}

This study attempts to cover breadth, not depth in possible indices. Each fishery has unique ecological characteristics that interact with environmental variables in different and non-linear ways. By studying a wide collection of fisheries and environmental variables we can uncover the potential feasibility of index insurance for fisheries holistically, and then further refine measures with ecologically sound models in future applications. 

### Fishery Data

Landings revenue, and participation data comes from the West Coast Fish data package [@Free2022]. It is a reconstruction of California Department of Fish and Wildlife catch data combined with PacFin receipts for Washington and Oregon. The last three years of data are updated from the CDFW Marine Fisheries Data Explorer (MFDE). Names are matched to each species within the West Coast Fish data package.

We select California fisheries with a minimum of 30 years of consecutive, non-confidential catch records at both the state and port-complex level. Unclassified catch records are dropped i.e. "Other Sharks" and similar categories. Fisheries with an average revenue from 2010-2019 greater than \$100,000 at the state and \$75,000 at port-complex level are analyzed. Twenty four fisheries at the state level and 50 fisheries at the port complex level meet these criteria. These fisheries contain the most economically important fisheries in California and their mean values are shown in @tbl-fish-sum and at the port complex in @tbl-fish-port-sum.


Fisheries have complex spatial dynamics. Agriculture has clear, quantifiable impacts of weather in grids that are well suited for index insurance. Drought on a single farm directly leads to crop loss for that farm. Whether there is sufficient spatial coverage to identify impacts down to an individual farm remains a challenge in agriculture [@Leppert2021;@Dalhaus2016;@Stigler2024]. Fish and fishers can move thousand of miles in a given year, thus more consideration must be given to the location of weather impacts in fisheries. We spatially refine catch histories using the California CDFW fishing blocks records from the MFDE Data Explorer. Summarized catch histories of all landed fish within each block provide an average representation of effort for a given fishery. Spatial catch history is measured at both the state and port-complex level. The spatial location refines the location of environmental variables. Local weather is more likely to affect fishery productivity and catch than observations thousands of miles away.

#### Uncollected Data

While the fishery data from CDFW is the most comprehensive available, it is not without its limitations. There are other data needs that may be relevant for the analysis that I have not collected yet. All of these data sources could add to the uniqueness of fishery index insurance.

Catch per unit effort data may be valuable as it might be able to indicate loss more effectively than harvest measures. Aggregate harvest measures are sensitive to the total amount of fishers in a given year. The variation in fishers can lead to large swings in catch data that are not related to weather. Catch per unit effort data is often used as a direct calculation of the underlying biomass, which will be affected by the weather. Individual catch histories with records days fished would be the most ideal data source, but I would need approval through the CDFW to access. 


Management covariates may be needed to indicate the relative influence of changes in regulation or weather to productivity. Quotas would be an excellent way to account for limitations on catch, but not all fisheries in the data set are managed through quotas. Instead other regulations could account for the weather independent effects. For example, Dungeness Crabs are managed through trap and bot limits that have changed over the years in addition to delays in season openings. A time series of management covariates would allow the models to separate institutional influences from weather influences. PacFin possesses current quota data and the stock assessments used in their determination.

Biomass estimates could be the most direct estimate of fishery productivity. Stock assessments are the most common way to estimate biomass, but they are not available for all fisheries. The Pacific Fisheries Management Council (PFMC) provides stock assessments for many of the fisheries in the data set, but not all. Additionally, some measures of biomass are built off of CPUE so I would need to avoid double counting those effects in the model. 


### Environmental Data

Fisheries are highly sensitive to marine heatwaves and water temperature. Sea surface temperature is a natural variable to first consider in fisheries index insurance. Sea surface temperature data comes from the NOAA DHW data set that provides 5-km resolution of monthly temperature from 1985 to 2023. The 5-km grids are averaged within the nearest California fishing block to provide an annual time series of temperature for each fishery. Temperature is lagged from 1 to 3 years prior to account for residual impacts that carry over due to fishery biological dynamics.

Upwelling provides vital nutrients to stimulate primary productivity. The coast of California is a highly productive ecosystem due to its patterns of upwelling [@Huyer1983;@Chelton1982]. We capture upwelling through monthly observations of Coastal Upwelling Transport Index (CUTI) and Biological Effective Upwelling Transport Index (BEUTI). Both indices create measures of vertical movement in the mixed layer at 1 degree latitude intervals extending 75 km along the entire US West Coast [@Jacox2018]. The closest layer to the surface was used in this analysis as the correlation between surface index values and deeper index values are high. CUTI examines the physical measures of wind, Ekman transport, and cross-shore geostrophic transport to indicate the strength of upwelling in a given month. BEUTI adds nitrate concentration in its calculation to capture more biological effects of upwelling. Fishing blocks are matched to the nearest 1 degree latitude interval to provide a monthly time series of upwelling for each fishery. Seasonal strengths of upwelling are captured by averaging CUTI and BEUTI within each quarter of the year. Spring upwelling in early March and April are especially important to a wide array of fish species. Yearly average and amplitude values (the difference between minimum observed upwelling and maximum) are also calculated. These indices are the most temporally limited datasets in this analysis, only extending from 1988 to 2023.

The Habitat Compression Index measures the area extent of water below average temperatures thresholds along the US West Coast [@Schroeder2022]. Habitat compression is a measure of the spatial extent of cold water habitats that are important for fish species. The index is broken down into four distinct oceangraphic regions ranging from 3.5 degrees to 5.5 degrees latitude in size with coverage out to 150 km offshore. We use the cumulative habitat compression index that sums the index value in each month to provide a yearly time series of habitat compression for each fishery. The cumulative index showed stronger correlations with biological productivity measures than monthly measures [@Schroeder2022]

The final environmental variables are the Pacific Decadal Oscillation (PDO) and the El Nino Southern Oscillation (ENSO). Both indices are well known to affect marine ecosystems and fisheries. Both indices are averaged over a given year. PDO data is taken from the PDO ERSST V5, and ENSO data is taken from the multivariate ENSO Index Version 2 (MEI.v2).

Summary statistics for the environmental data are presented in @tbl-env-sum. In total, 74 fisheries with 35 years of catch data and 20 weather variables are spatially matched with annual coverage from 1988 to 2023.

```{r}
library(tidyverse)
#load weather data
load(here::here("data","environmental","block_beuti.rda"))
load(here::here("data","environmental","block_cuti.rda"))
load(here::here("data","environmental","block_hci.rda"))
load(here::here("data","environmental","block_sst.rda"))
load(here::here("data","environmental","enso_pdo.rda"))

#load designed functions
source(here::here("src","fcn","cw_join_cali.R"))
```


## Methods {#sec-methods}

We use three models to predict yearly fishing revenue and landings at state and port-complex levels. Linear models are used as the base model given its ubiquitous use in index insurance policies. We compare utility improvements with the adoption of more robust LASSO regression and random forest models.

In all class of models, the final utility maximization choice of coverage leverage is found through a box constrained quasi-Newton Method using the optim function in R. Choices are constrained to be non-negative. Premium schedules are found by the model output below the trigger values in @eq-payout and then averaged over the total fishery data. A root finder then determines what $m$ will equate the expected utility of having insurance with the expected utility of not having insurance.

### Linear Models

Perfect regression coefficients mimic the optimal choice of scale in index insurance contracts [@Mahul1999]. Combined with the ease of implementation, linear models on single weather indices are the most common design choice for index insurance policies. They offer a basic starting place to consider the viability of fishery index insurance. 

Yearly aggregated fishing variables are regressed on each environmental variable individually. Weather variables are spatially matched to the location of catch. We perform a 10 fold cross validation method to determine the best individual weather variable based on root mean square error (RMSE). To preserve the time series element of the data, we used a rolling split to partition the training and testing data. For example, the first fold contains the first 70% of data as training (1988-2011), and the last 30% as testing (2013-2023). The final date of the training set is extended in each fold until the year 2020 to create 10 folds. Models with the lowest average RMSE are selected and trained on the full set before being passed to the utility optimization procedure in @eq-utility. 


### LASSO Regression

Least Absolute Shrinkage and Selection Operator (LASSO) regression is a popular regularization technique to assist model selection. It attempts to minimize the residual sum of squared errors through Ordinary Least Squares (OLS), but adds a penalty constraint on the absolute sum of selected coefficient values (@eq-lasso).

$$
\hat{\beta}^{lasso}=\arg\min_{\beta}\left\{\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\omega_{ij}\beta_j)^2+\lambda\sum_{j=1}^{p}|\beta_j|\right\}
$$ {#eq-lasso}

Where, $y_i$ is our fishing variable, $\beta$ the regression coefficients, $n$, the number of observations, $p$ the number of predictors, and $\omega$ the total collection of weather variables. The $\lambda$ is the penalty term that controls the amount of shrinkage. Models are trained using the `glmnet` package in R. The LASSO regression model is trained on 200 bootstrapped samples of the training data. The optimal $\lambda$ is selected through a grid search method that selects the minimum RMSE. This choice is to ensure the most parsimonious model that still captures the most important weather variables. LASSO is particularly well suited for this research design as the absolute value of the penalty term shrinks coefficients to zero. Overfitting is a concern with so few observations in the initial training set; the shrinkage towards zero will help minimize this bias by reducing the parameter space.


### Random Forests

While LASSO offers us the ability to simultaneously explore a wide collection of weather variables including lagged effects, it remains linear in its predictions. Random Forests are tree-based ensemble models that capture non-linear interactions through recursive partitioning. They are less sensitive to over fitting through the aggregation of many trees.  

We tune two hyperparameters to create the best performing random forest for each fishery: The number of variables to consider at each split and the minimum number of observations in a leaf node. We use a grid search method to find the best hyperparameters based on RMSE through the year based cross validation method presented in the linear models. The final model is trained on the full dataset and passed to the utility optimization procedure in @eq-utility.

### Weather variables of importance

Machine learning algorithms are inherently "black boxes" that sacrifice interpretability for predictive accuracy. Fishers will be less likely to purchase complicated products that do not correspond to their experiences. Extracting the relative contribution of weather variables will assist translating products to fishers. Additionally, it can help ground-truth the chosen variables with previous biological modelling. 

The cross-validation in the linear models provides a simple weather variable comparison. We calculate the frequency a given weather variable is chosen as the best performing linear model.

We use `vip` package in R to extract importance measures for both the LASSO and random forests ^[Nathan note: I need to read more exactly how this package will extract between permutations or variance measures]. Feature extraction will occur for each fishery product, and the importance of each will be normalized then aggregated in order to compare all features. 

## Preliminary Results {#sec-results}

Before running the analysis on all models, we examine only the difference between linear models and random forests for the state wide fishery data.  Preliminary results indicate there exists a fundamental difference between the training and testing datasets that is causing the random forest models to overfit. The linear models generally outperform the random forests in the testing data. The LASSO models are performing similarly to the linear models. The random forests are capturing the training data well, but are not generalizing to the testing data. The random forests are overfitting to the training data.

```{r}
#| fig-cap: "Root Mean Square Error performance is slighty better for linear models than random forests in the testing set. Random forests are overfitting both landings (blue) and revenue (green) in the training set. "
#| label: fig-rmse


load(here::here('data','output','cali_lm_ut_wtp.rda'))
load(here::here('data','output','cali_rf_ut_wtp.rda'))
load(here::here('data','output','port_lm_ut_wtp.rda'))
load(here::here('data','output','port_rf_ut_wtp.rda'))

# get train_rmse for rf and compare it relative to the lm rmse for both test and train

c_mt_train<-(cali_mt_rf_ut$train_rmse-cali_mt_lm_ut$train_rmse)/cali_mt_lm_ut$train_rmse


p_mt_train<-(port_mt_rf_ut$train_rmse-port_mt_lm_ut$train_rmse)/port_mt_lm_ut$train_rmse

rf_mt_train<-mean(c(c_mt_train,p_mt_train))

c_mt_test<-(cali_mt_rf_ut$test_rmse-cali_mt_lm_ut$test_rmse)/cali_mt_lm_ut$test_rmse

p_mt_test<-(port_mt_rf_ut$test_rmse-port_mt_lm_ut$test_rmse)/port_mt_lm_ut$test_rmse

rf_mt_test<-mean(c(c_mt_test,p_mt_test))

# Repeat same steps but with the rev 

c_rev_train<-(cali_rev_rf_ut$train_rmse-cali_rev_lm_ut$train_rmse)/cali_rev_lm_ut$train_rmse

p_rev_train<-(port_rev_rf_ut$train_rmse-port_rev_lm_ut$train_rmse)/port_rev_lm_ut$train_rmse

rf_train_rev<-mean(c(c_rev_train,p_rev_train))

c_rev_test<-(cali_rev_rf_ut$test_rmse-cali_rev_lm_ut$test_rmse)/cali_rev_lm_ut$test_rmse

p_rev_test<-(port_rev_rf_ut$test_rmse-port_rev_lm_ut$test_rmse)/port_rev_lm_ut$test_rmse

rf_test_rev<-mean(c(c_rev_test,p_rev_test))


plot_df<-data.frame(rmse=c(rf_mt_train,rf_mt_test,rf_train_rev,rf_test_rev),x=c('Train','Test','Train','Test'),type=c('Landings','Landings','Revenue','Revenue'))

ggplot(plot_df)+
  geom_col(aes(x=x,y=rmse,fill=type),position = 'dodge')+
  scale_fill_manual(name='',values=c('#047C91','#79A540'))+
  scale_y_continuous(labels = scales::percent)+
  theme_classic()+
  labs(x='',y='RMSE Relative to Linear Model')+
  theme(legend.position = 'top',
        title = element_text(size=19),
        axis.text=element_text(size=15),
        legend.text = element_text(size=12),
        legend.title = element_text(size=13))

```


The lack of management covariates is probably the primary cause of the discrepancy. Random forests are robust to overfitting, but if the are fundamental differences in the testing and training sets, or omitted variables, random forests can lead to poor out of sample performance. @fig-cab provides a clear demonstration of how management could be affecting the models. Cabezon had little management and became overfished in the early 2000s. The Pacific Fisheries Management Council implemented a quota system in the mid 2000s to help the stock recover that remains in place today. The peak of catch in the late 1990s corresponded to the strong 1997 El Nino event with high sea surface temperatures. The random forests attempt to capture similar warm water events in the 2015-2016 blob and drastically overestimate the predicted catch. The random forests could not properly capture the management effects of the quota. 


```{r}
#| fig-cap: "Measured Cabezon landings in metric tons from 1988-2023 (blue line). Predictions from random forestes (green points) perform well in the training sample compared to the linear model predictions (blue points). However, in the testing period post 2013 (dashed vertical red line) there is a distinct misalignment in predictions."
#| label: fig-cab

library(ranger)
load(here::here('data','output','cali_lm_models.rda'))
load(here::here('data','output','cali_rf_models.rda'))

load(here::here('data','fisheries','cali_catch.rda'))

load(here::here('data','fisheries','cali_port.rda'))

dcrb_data<-cali_mt_lm |> 
  filter(species_code=='CBZ1') |> 
  ungroup() |> 
  dplyr::select(cw_data) |> 
  unnest(cw_data)

lm_mod_df<-cali_mt_lm |> 
  filter(species_code=='CBZ1') |> 
  hoist(model,'final_mod','best_rmse')

lm_mod<-lm_mod_df$final_mod[[1]]

filt<-lm_mod_df$best_rmse$var

lm_pred<-dcrb_data |> filter(var==filt & fish_var=='landings_mt') |> 
  (\(x)(predict(lm_mod,newdata=x)))()


rf_mod_df<-cali_mt_rf |> 
  filter(species_code=='CBZ1')

rf_mod<-rf_mod_df$model[[1]]$final_mod

rf_pred<-dcrb_data |> filter(fish_var=='landings_mt') |> 
  pivot_wider(
        names_from=var,
        values_from=value
      ) |> 
  drop_na() |> 
  (\(x)(predict(rf_mod,x)$predictions))()


dcrb<-dcrb_data |> 
  filter(fish_var=='landings_mt') |> 
  pivot_wider(
        names_from=var,
        values_from=value
      ) 
rf_pred<-c(NA,rf_pred)
ggplot(data=dcrb)+
  geom_line(aes(x=year,y=fish_value,color='Data'),linewidth=2)+
  geom_point(aes(x=year,y=lm_pred,color='Linear Model'),size=3)+
  geom_point(aes(x=year,y=rf_pred,color='Random Forest'),size=3)+
  scale_color_manual(name="",values=c('Data'='#003660','Linear Model'='#047C91','Random Forest'='#79A540'))+
  theme_classic()+
  labs(x='Year',y='Landings (MT)',title = "Cabezon")+
  theme(legend.position = 'top')+
  geom_hline(yintercept=mean(dcrb$fish_value),linetype='dashed',color='black',linewidth=2)+
  theme(title = element_text(size=14),
        axis.text=element_text(size=12),
        legend.text = element_text(size=12),
        legend.title = element_text(size=14))+
  geom_vline(xintercept=2013,linetype='dashed',color='red',size=1.5)
  

  
```

The performance of the utility model is biased by the underlying predicted model performance, but also demonstrate how average strike levels may be problematic. 

```{r}
#| fig-cap: "The marginal willingness to pay for insurance based on a linear model. Fishery species code on the x-axis."
#| label: fig-ut
cali_mt_lm_ut %>% 
  ggplot()+
  geom_col(aes(x=species_code,y=m),fill='#003660')+
  scale_y_continuous(expand=c(0,0))+
  theme_classic()+
  labs(x='',y='M (Willingness to Pay')+
  theme(axis.text.x = element_text(angle=45,hjust=1))
```

```{r}
#| fig-cap: "The marginal willingness to pay for insurance based on a random forest model. Fishery species code on the x-axis."
#| label: fig-utrf
cali_mt_rf_ut %>% 
  ggplot()+
  geom_col(aes(x=species_code,y=m),fill='#003660')+
  scale_y_continuous(expand=c(0,0))+
  theme_classic()+
  labs(x='',y='M (Willingness to Pay)')+
  theme(axis.text.x = element_text(angle=45,hjust=1))
```


```{r}
#| fig-cap: "A moving average (green) may be able to more closely match the underlying biological dynamics of the fishery than the historical average (blue). The moving average of landings in metric tons for Albacore and Dungeness Crab."
#| label: fig-avg

library(zoo)
library(patchwork)

cali_cw<-cali_catch %>% 
  group_by(species_code) %>%
  mutate(mt_per_fisher=case_when(mt_per_fisher==Inf~0,
                              .default=as.numeric(mt_per_fisher)),
         lb_per_fisher=case_when(lb_per_fisher==Inf~0,
                                 .default=as.numeric(lb_per_fisher))) %>% 
  mutate(roll_value_usd=rollmean(value_usd,3,fill=NA,align="right",na.rm=TRUE),
         roll_landings=rollmean(landings_mt,3,fill=NA,align='right',na.rm=TRUE),
         roll_n_rev=rollmean(rev_per_fisher,3,fill=NA,align='right',na.rm=TRUE),
         roll_n_mt=rollmean(mt_per_fisher,3,fill=NA,align='right',na.rm=TRUE)) %>%
  filter(year>=1988) 

## Plot albc

p5<-cali_cw %>% filter(species_code=='ALBC') %>% 
  ggplot()+
  geom_point(aes(x=year,y=landings_mt,color='Data'),size=3)+
  geom_line(aes(x=year,y=mean(landings_mt),color='Mean'),size=2)+
  geom_line(aes(x=year,y=roll_landings,color='Moving Average'),size=2)+
  scale_color_manual(values=c('black','blue','forestgreen'))+
  theme_classic()+labs(x='',y='Landings (MT)',title='Albacore')+
  theme(legend.title=element_blank())

p6<-cali_cw %>% filter(species_code=='DCRB') %>% 
  ggplot()+
  geom_point(aes(x=year,y=landings_mt,color='Data'),size=3)+
  geom_line(aes(x=year,y=mean(landings_mt),color='Mean'),size=2)+
  geom_line(aes(x=year,y=roll_landings,color='Moving Average'),size=2)+
  scale_color_manual(values=c('black','blue','forestgreen'))+
  theme_classic()+labs(x='',y='Landings (MT)',title='Dungeness Crab')+
  theme(legend.title=element_blank())

p5+p6+ plot_layout(guides = 'collect',axis_titles = 'collect')
```


## Future Steps {#sec-discussion}

- **Variable importance**

Once the performance of the models is refined with better data or alternative testing-training splits, the trained models will indicate which weather variables are the most important to predicting catch or revenue. For example, the linear models that had the best performance in the cross validation show upwelling indices have the strongest relationship with catch. Analyzing the variable importance will help ground interpretation of the models for both fishers and fishery scientists. Describing black-box machine learning models will be an uptake barrier that can be alleviated by breaking down the model into more interpretable terms. 

```{r}
#| fig-cap: "The number of times a weather variable was selected as the best predictor in the linear models. "
#| label: fig-var
load(here::here('data','output','cali_lm_models.rda'))

cali_mt_lm %>% 
hoist(model,'best_rmse') %>% 
  ggplot()+
  geom_bar(aes(x=best_rmse$var),fill='royalblue')+
  theme_classic()+
  labs(x='',y='Times selected as best predictor')+
  theme(axis.text.x = element_text(size=14,hjust=1,angle=45),
        axis.text.y = element_text(size=14),
        title = element_text(size=16),
        axis.title = element_text(size=14))
```


- **Use management measurements**

The performance of the models is concerning without accounting for fundamental differences in the training and testing sets. Once I have collected some indicator of management the prediction models will need to be retrained for each productivity measure. Perhaps even a simple dummy variable will help the models perform better without the explicit measures of management.



- **Trigger on biomass**

The weather data collected connects to biological productivity. Biomass could be a more direct measure of the underlying fishery health without the tenuous connections between weather and catch. The amount of fish is also an input so it will correlate tightly with catch. Estimates of biomass may not exist for all fisheries. I can trim down the analysis to just those that have stock abundance estimates over time.  Catch per unit effort does serve as a proxy for biomass, so using CPUE for $\pi$ may suffice.


- **Compare between different contracts**

So far, I've only analyzed the utility with contracts based on long run average. AS described above, these may be problematic. The next step needs to use different trigger measures. However, without resolving the difference in the training and testing sets, it may be moot if the underlying models perform poorly.

- **Analyze payouts and insurance company profits**

The high demand present for some linear models stems from consistent payouts in recent years. Insurance companies may be forced to operate at significant losses if they have to make reoccurring payments. A robustness check would be the verify the premium an insurance company would need to charge to make a profit and compare to the willingness to pay estiamted for the fishers. 


\newpage
\appendix
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\setcounter{table}{0}

## Appendix {.appendix}

```{r}
library(tidyverse)

load(here::here("data","fisheries","cali_catch.rda"))
load(here::here("data","fisheries","cali_port.rda"))
```



```{r}
#| label: tbl-fish-sum
#| tbl-cap: Summary statistics of catch from 1988-2023 for California fisheries.

library(kableExtra)



sum_tbl<-cali_catch %>% 
  mutate(mt_per_fisher=case_when(mt_per_fisher==Inf~0,
                              .default=as.numeric(mt_per_fisher)),
         lb_per_fisher=case_when(lb_per_fisher==Inf~0,
                                 .default=as.numeric(lb_per_fisher))) |> 
  group_by(comm_name) %>% 
  summarize(mean_landings_mt=mean(landings_mt),
            sd_landings=sd(landings_mt),
            mean_value_usd=mean(value_usd),
            sd_value=sd(value_usd),
            mean_mt_per_fisher=mean(mt_per_fisher),
            sd_per=sd(mt_per_fisher,na.rm=TRUE),
            n=mean(n_fisher),
            n_sd=sd(n_fisher)) %>% 
  mutate(across(where(is.numeric),round,1)) |> 
  mutate(mean_value_usd=scales::dollar(mean_value_usd))

sum_tbl |> 
  kable(format='latex',col.names = c("Species","Mean","SD","Mean","SD","Mean","SD","Mean","SD"),booktabs=T) |> 
kable_styling(latex_options = c("scale_down")) |>
  add_header_above(c(" ","Landings (mt)"=2,"Revenue (USD)"=2,"MT per Fisher"=2,"Number of Fishers"=2))


```


```{r}
#| label: tbl-fish-port-sum
#| tbl-cap: Summary statistics of catch from 1988-2023 for California fisheries split between species and port complex
# Make a summary table of the port complex fish data
library(kableExtra)

sum_tbl_port<-cali_port_catch %>% 
  mutate(mt_per_fisher=case_when(mt_per_fisher==Inf~0,
                              .default=as.numeric(mt_per_fisher)),
         lb_per_fisher=case_when(lb_per_fisher==Inf~0,
                                 .default=as.numeric(lb_per_fisher))) |> 
  group_by(spp_code,port_area) %>% 
  summarize(comm_name=unique(comm_name),
    mean_landings_mt=mean(landings_mt),
            sd_landings=sd(landings_mt),
            mean_value_usd=mean(revenues_usd),
            sd_value=sd(revenues_usd),
            mean_mt_per_fisher=mean(mt_per_fisher),
            sd_per=sd(mt_per_fisher,na.rm=TRUE),
            n=mean(n_fisher),
            n_sd=sd(n_fisher)) %>% 
  mutate(across(where(is.numeric),round,1)) |> 
  mutate(mean_value_usd=scales::dollar(mean_value_usd)) |> 
  drop_na(comm_name) |> 
  mutate(port_area=stringr::str_to_title(port_area)) |> 
  ungroup() |> 
  select(comm_name,port_area,everything()) |> 
  select(-spp_code)

sum_tbl_port |> 
  kable(format='latex',booktabs=T,col.names = c("Species","Port","Mean","SD","Mean","SD","Mean","SD","Mean","SD")) |> 
  #collapse_rows(columns=1:2,valign="top") |> 
  kable_styling(latex_options = c("scale_down")) |> 
   add_header_above(c(" "=2,"Landings (mt)"=2,"Revenue (USD)"=2,"MT per Fisher"=2,"Number of Fishers"=2))

```


```{r}
#| label: tbl-env-sum
#| tbl-cap: Summary statistics of environmental variables from 1988-2023 for California fisheries.

load(here::here("data","environmental","block_cuti.rda"))
load(here::here("data","environmental","block_beuti.rda"))
load(here::here("data","environmental","block_hci.rda"))
load(here::here("data","environmental","block_sst.rda"))
load(here::here("data","environmental","enso_pdo.rda"))


# separate each into yearly mean, yearly sd, the temporal resolution and the spatial resolution

cuti_sum<-block_cuti %>% 
  drop_na() |> 
  group_by(var) %>% 
  summarize(mean=mean(value),
            sd=sd(value)) %>% 
  mutate(resolution="Monthly",
         spatial="1 degree latitude",
         source="Jacox et al., 2018") |> 
  select(-var)

rownames(cuti_sum)<-c("CUTI Amp","CUTI Avg","CUTI Fall","CUTI Summer","CUTI Spring","CUTI Winter")

beuti_sum<-block_beuti %>% 
  drop_na() |> 
  group_by(var) %>% 
  summarize(mean=mean(value),
            sd=sd(value)) %>% 
  mutate(resolution="Monthly",
         spatial="1 degree latitude",
         source="Jacox et al., 2018") |> 
  select(-var)

rownames(beuti_sum)<-c("BEUTI Amp","BEUTI Avg","BEUTI Fall","BEUTI Summer","BEUTI Spring","BEUTI Winter")

hci_sum<-block_hci %>% 
  drop_na() |> 
  group_by(var) %>% 
  summarize(mean=mean(value),
            sd=sd(value)) %>% 
  mutate(resolution="Yearly",
         spatial="1 degree latitude",
         source="Integrated Ecosytem Assessment") |> 
  select(-var)

rownames(hci_sum)<-c("Cummulative Habitat Compression Index")

sst_sum<-block_sst %>% 
  drop_na() |> 
  group_by(var) %>% 
  summarize(mean=mean(value),
            sd=sd(value)) %>% 
  mutate(resolution="Monthly",
         spatial="5x5 km",
         source="NOAA Coral Bleaching Degree Heating Week") |> 
  select(-var)

rownames(sst_sum)<-c("Average Sea Surface Temperature","Sea Surface Temperature Lag 1 Year","Sea Surface Temperature Lag 2 Years","Sea Surface Temperature Lag 3 Years","Sea Surface Temperature Lag 4 Years")

enso_sum<-enso |> 
  group_by(year) |> 
  summarize(yr_enso=mean(enso)) |> 
  ungroup() |> 
  drop_na() |> 
  summarize(mean=mean(yr_enso),
         sd=sd(yr_enso)) |> 
  mutate(resolution="Monthly",
         spatial="Regional",
         source="MEI.v2")

rownames(enso_sum)<-c("ENSO")

pdo_sum<-pdo |> 
  group_by(year) |> 
  summarize(yr_pdo=mean(pdo)) |> 
  ungroup() |> 
  drop_na() |> 
  summarize(mean=mean(yr_pdo),
         sd=sd(yr_pdo)) |> 
  mutate(resolution="Monthly",
         spatial="Regional",
         source="PDO ERSST V5")

rownames(pdo_sum)<-c("Pacific Decadal Oscillation")

env_tbl<-rbind(cuti_sum,beuti_sum,hci_sum,sst_sum,enso_sum,pdo_sum) |> 
  mutate(across(where(is.numeric),round,1))

env_tbl$variable<-c("CUTI Amp","CUTI Avg","CUTI Fall","CUTI Summer","CUTI Spring","CUTI Winter",
                     "BEUTI Amp","BEUTI Avg","BEUTI Fall","BEUTI Summer","BEUTI Spring","BEUTI Winter",
                     "Cummulative Habitat Compression Index",
                     "Average Sea Surface Temperature","Sea Surface Temperature Lag 1 Year","Sea Surface Temperature Lag 2 Years","Sea Surface Temperature Lag 3 Years","Sea Surface Temperature Lag 4 Years",
                     "ENSO","Pacific Decadal Oscillation")

env_tbl<-env_tbl %>%
  select(variable,mean,sd,resolution,spatial,source)



env_tbl |>
  kable(format='latex',col.names = c("Weather Index","Mean","SD","Temporal Resolution","Spatial Resolution","Source"),booktabs=T) |>
  kable_styling(latex_options = "scale_down")


```


## References
